{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 finalplz.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def crawl_all_pages_selenium(search_query, start_date, end_date, total_pages=100):\n",
    "    output_filename = \"finalplz.csv\"\n",
    "\n",
    "    # 웹드라이버 설정\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless\")  # 헤드리스 모드 비활성화\n",
    "    options.add_argument(\"--disable-gpu\")  # Optional: Disable GPU hardware acceleration\n",
    "    options.add_argument(\"--no-sandbox\")  # Optional: Disable the sandbox for service workers.\n",
    "\n",
    "    # Firefox specific settings\n",
    "    options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "    options.set_preference('useAutomationExtension', False)\n",
    "    options.set_preference(\"privacy.trackingprotection.enabled\", True)\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['page', 'title', 'url', 'date']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for current_page in range(1, total_pages + 1):\n",
    "            url = f'https://search.naver.com/search.naver?where=news&query={search_query}&sm=tab_pge&sort=0&pd=3&ds={start_date}&de={end_date}&start={(current_page - 1) * 10 + 1}'\n",
    "            driver.get(url)\n",
    "\n",
    "            # 웹 페이지에서 데이터 추출\n",
    "            news_items = driver.find_elements(By.CSS_SELECTOR, 'div.news_area')\n",
    "            for news_item in news_items:\n",
    "                title_selector = 'a.news_tit'\n",
    "                date_selector = 'div.info_group span.info'\n",
    "                url_selector = 'a.news_tit'\n",
    "\n",
    "                try:\n",
    "                    title = news_item.find_element(By.CSS_SELECTOR, title_selector).get_attribute('title')\n",
    "                    url = news_item.find_element(By.CSS_SELECTOR, url_selector).get_attribute('href')\n",
    "                    date = news_item.find_element(By.CSS_SELECTOR, date_selector).text\n",
    "                except NoSuchElementException:\n",
    "                    # Skip if any of the elements is not found\n",
    "                    continue\n",
    "\n",
    "                # Write to CSV with page information\n",
    "                writer.writerow({'page': current_page, 'title': title, 'url': url, 'date': date})\n",
    "\n",
    "            # 딜레이 추가\n",
    "            time.sleep(1)  # 1초 딜레이 (원하는 시간으로 변경)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"전동 킥보드\"\n",
    "    start_date = \"2023.01.01\"\n",
    "    end_date = \"2024.04.26\"\n",
    "\n",
    "    crawl_all_pages_selenium(search_query, start_date, end_date)\n",
    "    print(\"데이터가 finalplz.csv 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL에 대한 기사 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (10.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (6.0.1)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (2.31.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/donggunhan/Documents/Visual Studio Code/.venv/lib/python3.11/site-packages (from tldextract>=2.0.1->newspaper3k) (3.12.4)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=cdaf841266027ef6a40e38d378f30c0a69dfae1e145dd1692f2336112615844c\n",
      "  Stored in directory: /Users/donggunhan/Library/Caches/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=dc035aa35b9685d83256ff630c3eccc3607015d9c375b3d27ab5cf0deb5c9b9d\n",
      "  Stored in directory: /Users/donggunhan/Library/Caches/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=1295eca2a4c8b12e947e505780ca877f120c6252731bd51204da2b7d2c6e8ea0\n",
      "  Stored in directory: /Users/donggunhan/Library/Caches/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=923032e67f5bfac07d394beae5d29cfa608f8642b4dd75bff4c43e0612bf6b3a\n",
      "  Stored in directory: /Users/donggunhan/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스 내용 추출 중 오류 발생: Article `download()` failed with 403 Client Error: Forbidden for url: http://news.maxmovie.com/437606 on URL http://news.maxmovie.com/437606\n",
      "뉴스 내용 추출 중 오류 발생: Article `download()` failed with HTTPConnectionPool(host='news.mbccb.co.kr', port=80): Read timed out. on URL http://news.mbccb.co.kr/home/sub.php?menukey=61&mod=view&RECEIVE_DATE=20240123&SEQUENCE=4626\n",
      "데이터가 1000news_data.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from newspaper import Article\n",
    "\n",
    "def extract_news_content(url):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.text, article.publish_date\n",
    "\n",
    "def process_csv_with_newspaper(input_csv_filename, output_csv_filename):\n",
    "    with open(input_csv_filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        with open(output_csv_filename, 'w', newline='', encoding='utf-8') as output_csvfile:\n",
    "            fieldnames = ['page', 'title', 'url', 'date', 'content']\n",
    "            writer = csv.DictWriter(output_csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                url = row['url']\n",
    "\n",
    "                # Extract news content and date using newspaper3k\n",
    "                try:\n",
    "                    content, publish_date = extract_news_content(url)\n",
    "                except Exception as e:\n",
    "                    print(f\"뉴스 내용 추출 중 오류 발생: {e}\")\n",
    "                    content, publish_date = '', None\n",
    "\n",
    "                # Write to the output CSV file\n",
    "                writer.writerow({\n",
    "                    'page': row['page'],\n",
    "                    'title': row['title'],\n",
    "                    'url': url,\n",
    "                    'date': publish_date,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update file paths based on your directory structure\n",
    "    input_csv_filename = \"finalplz.csv\"\n",
    "    output_csv_filename = \"1000news_data.csv\"\n",
    "\n",
    "    process_csv_with_newspaper(input_csv_filename, output_csv_filename)\n",
    "    print(f\"데이터가 {output_csv_filename} 파일로 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
